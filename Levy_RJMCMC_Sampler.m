function [samples,accept] = Levy_RJMCMC_Sampler(y, X, J_0, theta_0, numSamples, proposalStepSize, LevyPrior_0, BasisFunction, sigma2, MoveProb, SKIParams)
% Fully Bayesian Reversible Jump Metropolis-Hastings sampler for
% inference on an input function assumed to be generated by a GP. The
% underlying kernel is estimated.
%
% Incorporates with Structured Kernel Interpolation (SKI) to handle larger
% training data sets than can be directly computed
%
% Inputs:
% y - N x 1
%   Noisy function to be estimated
% X - N x 1
%   Points at which y is evaluated
% J_0 - integer
%   Initial number of basis functions in presumed expansion of y
% theta_0 - 3 x J_0 matrix
%   Initial parameters
% numSamples - integer
%   Number of MCMC samples to generate
% proposalStepSize - 5 x 1 vector
%   Width of proposal distributions
% LevyPrior - structure
%   LevyPrior.name - string: Gamma, SymGamma, or Stable
%   LevyPrior.hyperparams - vector: [gam, eta, epsilon] or [gam, eta, epsilon, alpha]
%      gam controls sparsity of the expansions by scaling the mean of J in Levy prior
%        --The value supplied in input is used as the initial gamma
%      eta inversely controls the mean coefficient size and therefore the controls the maximum
%           height of basis functions. (smaller eta -> larger average coefficients)
%        --The value supplied in input is used as the initial eta
%      epsilon is the lower truncation bound on coefficient size to approximate the infinitely many
%           jumps in the Levy prior. It is set based on tolerated L2 error
%      alpha controls the heavy tails of the coefficients in the Stable process. It is between 1
%           and 2, with 1 having the heaviest tails (Cauchy tails) and 2 having Gaussian tails
%   LevyPrior.hyperhyperparams - vector: [a_gam, b_gam, a_eta, b_eta]
%      gam ~ Gamma(a_gam, b_gam)
%      eta ~ Gamma(a_eta, b_eta)
%   LevyPrior.betalogscale = 1 if MCMC is done on log(beta)
%                            0 if MCMC is done on beta
%
% BasisFunction - structure
%   BasisFunction.domain - 2 x 1: [xmin, xmax]
%   BasisFunction.function - function handle
%   BasisFunction.function_ift - inverse fourier transform handle
%   BasisFunction.hyperparams - vector: [a_lambda, b_lambda]
%      a_lambda controls skewness of basis function length scale samples
%      b_lambda inversely controls the mean size of basis function length scales (smaller b_lambda -> wider basis functions)
%   BasisFunction.lambdalogscale = 1 if MCMC is done on log(lambda)
%                                = 0 if MCMC is done on lambda
% sigma2 - scalar
%   Supposed noise variance
% MoveProb - 4 x 1, sum(MoveProb(1:3)) = 1
%    MoveProb(1) = Birth Probability
%    MoveProb(2) = Death Probability
%    MoveProb(3) = Update Probability
%    MoveProb(4) = Hyperparameter Update Probability
%
% SKIParams - structure (optional - if not supplied, then an exact calculation is made)
%   SKIParams.bf_ift_wrapper - handle: wrapper function of basis function ift to make it compatible with gpml
%   SKIParams.xg - vector: Grid points for SKI
%   SKIParams.cg_maxit - scalar: Max Iterations for Conjugate Gradient to calculate (K+sigma2*I)\y
%   SKIParams.cg_tol - scalar: Residual Tolerance for Conjugate Gradient to calculate (K+sigma2*I)\y
%   SKIParams.cg_showit - scalar: Set to 1 to show number of iterations for CG to converge and assess cg_maxit
%   SKIParams.use_pred_var - scalar: Flag for Perturb-and-MAP (1 to activate, 0 to deactivate)
%   SKIParams.pred_var - scalar: Perturb-and-MAP parameter
% 
% Outputs:
% samples - structure
%    samples.theta sum(3*J) x 1 vector of expansion parameters
%    samples.J numSamples x 1 vector of number of basis functions
%    samples.gam numSamples x 1 vector of gam hyperparameter
%    samples.eta numSamples x 1 vector of eta hyperparameter
%    samples.log_Likelihood numSamples x 1 vector of log likelihoods
%    samples.log_Posterior numSamples x 1 vector of log likelihoods
% accept - numSamples x 6
%   Acceptance indicators

% If SKI parameters not supplied, then do a direct calculation.
if nargin < 11
    useSKI = 0;
else
    useSKI = 1;
end

N = length(y);

% Check if input locations X are equally spaced and compute pairwise differences tauX
eqspace = range(diff(X)) < 1e-8;
if eqspace
    tauX=linspace(0,(X(2)-X(1))*(length(X)-1),length(X))';
else
    tauX = repmat(X,[1,N])-repmat(X',[N,1]);
end

% Extract hyperparameters and hyperhyperparameters
gam_0 = LevyPrior_0.hyperparams(1);
eta_0 = LevyPrior_0.hyperparams(2);
epsilon = LevyPrior_0.hyperparams(3);
a_gam = LevyPrior_0.hyperhyperparams(1);
b_gam = LevyPrior_0.hyperhyperparams(2);
a_eta = LevyPrior_0.hyperhyperparams(3);
b_eta = LevyPrior_0.hyperhyperparams(4);
betalogscale = LevyPrior_0.betalogscale;
lambdalogscale = BasisFunction.lambdalogscale;

if useSKI == 1
    % Extract SKI Parameters
    bf_ift_wrapper = SKIParams.bf_ift_wrapper;
    xg = SKIParams.xg;
    cgopt.cg_maxit = SKIParams.cg_maxit;
    cgopt.cg_tol = SKIParams.cg_tol;
    if isfield(SKIParams,'cg_showit')
        cgopt.cg_showit = SKIParams.cg_showit;
    end
    if isfield(SKIParams,'use_pred_var') && SKIParams.use_pred_var == 1
        cgopt.pred_var = SKIParams.pred_var;
    end
end

% Initialize output
samples.theta = zeros(numSamples*J_0*3,1);
samples.theta(1:3*J_0) = theta_0(:);
samples.J = [J_0; zeros(numSamples-1,1)];
samples.gam = [gam_0; zeros(numSamples-1,1)];
samples.eta = [eta_0; zeros(numSamples-1,1)];

% 1. Compute initial probability of theta_0
% 1a) Compute likelihood
% Construct proposal function from parameters
BFParams = theta_0(2:end,:)';

% Compute log likelihood
if useSKI == 0
    % Direct calculation of log likelihood if SKI parameters not provided
    if betalogscale == 1
        beta = exp(theta_0(1,:)');
    else
        beta = theta_0(1,:)';
    end
    if eqspace
        K_Synth = toeplitz(BasisFunction.function_ift(tauX,BFParams)*beta) + sigma2*eye(N);
    else
        K_Synth = reshape(BasisFunction.function_ift(tauX(:),BFParams)*beta,N,N) + sigma2*eye(N);
    end
    C_Synth = cholcov(K_Synth + diag(1e-6*rand(length(K_Synth),1)));
    log_det = sum(log(diag(C_Synth)));
    data_fit = y'*(C_Synth \ (C_Synth' \ y))/2;
    current_log_Likelihood = -data_fit - log_det - N/2*log(2*pi);
    samples.log_Likelihood = [current_log_Likelihood; zeros(numSamples-1,1)];
else
    % SKI approximation of log likelihood
    gpcov = {bf_ift_wrapper,betalogscale,lambdalogscale,BFParams,theta_0(1,:)'}; 
    gpmean = {@meanZero}; 
    lik = {@likGauss};
    hyp.cov = [];
    hyp.mean = [];
    hyp.lik = 0.5*log(sigma2);
    covg = {@apxGrid,{gpcov},{xg}};
    [~,nlZg] = infGrid(hyp,gpmean,covg,lik,X,y,cgopt);
    current_log_Likelihood = -nlZg;
end
samples.log_Likelihood = [current_log_Likelihood; zeros(numSamples-1,1)];

% 1b) Compute prior
[current_theta_prior, current_J_prior] = Levy_Prior_PDF(LevyPrior_0,BasisFunction,J_0,theta_0);

Z = randn([numSamples,5]);
UH = rand([numSamples,3]);
accept = zeros(numSamples,6);
accept(1,:) = [1, 1, 1, 1, 1, 1];

Uhyp = rand([numSamples,1]);
Umove = rand([numSamples,1]);
Uidx = rand([numSamples,1]);
p_plus = MoveProb(1);
p_minus = MoveProb(2);
p_equal = MoveProb(3);
p_hyp = MoveProb(4);

current_J = J_0;
current_theta = theta_0;
current_LevyPrior = LevyPrior_0;
current_gam = gam_0;
current_eta = eta_0;
current_hyperprior = gampdf(gam_0, a_gam, 1./b_gam)*gampdf(1/eta_0, a_eta, 1./b_eta);
samples.log_Posterior = zeros(numSamples,1);
samples.log_Posterior(1) = current_log_Likelihood + sum(log(current_theta_prior(:))) + log(current_J_prior) + log(current_hyperprior);

idx_l = 3*J_0+1;
for s = 1:numSamples-1
    fprintf('\nSample %d:\n',s+1);
    if Uhyp(s) <= p_hyp
        %Hyperparameter update:
        proposed_LevyPrior = current_LevyPrior;
        proposed_gam = current_gam + proposalStepSize(4)*Z(s,4);
        proposed_eta = current_eta + proposalStepSize(5)*Z(s,5);
        proposed_LevyPrior.hyperparams(1) = proposed_gam;
        proposed_LevyPrior.hyperparams(2) = proposed_eta;
        
        proposed_hyperprior = gampdf(proposed_gam, a_gam, 1./b_gam)*gampdf(1/proposed_eta, a_eta, 1./b_eta);
        [proposed_theta_prior, proposed_J_prior] = Levy_Prior_PDF(proposed_LevyPrior,BasisFunction,current_J,current_theta);
        H = exp(sum(log(proposed_theta_prior(:))) - sum(log(current_theta_prior(:)))...
            + log(proposed_J_prior/current_J_prior)...
            + log(proposed_hyperprior/current_hyperprior));
        
        if H > UH(s,1)
            % Update Hyperparameters and Priors
            current_LevyPrior = proposed_LevyPrior;
            current_gam = proposed_gam;
            current_eta = proposed_eta;
            current_theta_prior = proposed_theta_prior;
            current_J_prior = proposed_J_prior;
            
            samples.gam(s+1) = proposed_gam;
            samples.eta(s+1) = proposed_eta;
            accept(s+1,:) = [4, 0, 0, 0, 1, 1];
        else
            samples.gam(s+1) = current_gam;
            samples.eta(s+1) = current_eta;
            accept(s+1,:) = [4, 0, 0, 0, 0, 0];
        end
        
        % Copy Current Parameters
        idx_u = idx_l + 3*current_J - 1;
        samples.theta(idx_l:idx_u) = current_theta(:);
        samples.J(s+1) = current_J;
        idx_l = idx_u + 1;
        
        
    else
        % If not hyperparameter update, then update other parameters
        
        % 2. Propose a new set of parameters
        % Determine which type of move
        if current_J == 0 || Umove(s) <= p_plus
            % Birth Step
            
            proposed_Value = Levy_Basis_Sampler(current_LevyPrior, BasisFunction);
            proposed_theta = [current_theta,proposed_Value];
            proposed_J = current_J + 1;
            
            [proposed_theta_prior, proposed_J_prior] = Levy_Prior_PDF(current_LevyPrior,BasisFunction,proposed_J,proposed_theta);
            qb = Levy_Prior_PDF(current_LevyPrior,BasisFunction,1,proposed_Value);
            
            proposed_BFParams = proposed_theta(2:end,:)';
            % Compute GP log likelihood
            if useSKI == 0
                % Direct calculation of log likelihood if SKI parameters not provided
                if betalogscale == 1
                    proposed_beta = exp(proposed_theta(1,:)');
                else
                    proposed_beta = proposed_theta(1,:)';
                end
                if eqspace
                    K_Synth = toeplitz(BasisFunction.function_ift(tauX,proposed_BFParams)*proposed_beta) + sigma2*eye(N);
                else
                    K_Synth = reshape(BasisFunction.function_ift(tauX(:),proposed_BFParams)*proposed_beta,N,N) + sigma2*eye(N);
                end
                C_Synth = cholcov(K_Synth + diag(1e-6*rand(length(K_Synth),1)));
                if isempty(C_Synth)
                    proposed_log_Likelihood = -inf;
                else
                    log_det = sum(log(diag(C_Synth)));
                    data_fit = y'*(C_Synth \ (C_Synth' \ y))/2;
                    proposed_log_Likelihood = -data_fit - log_det - N/2*log(2*pi);
                end
            else
                % SKI approximation of log-likelihood
                gpcov = {bf_ift_wrapper,betalogscale,lambdalogscale,proposed_BFParams,proposed_theta(1,:)'};
                covg = {@apxGrid,{gpcov},{xg}};
                try
                    [~,nlZg] = infGrid(hyp,gpmean,covg,lik,X,y,cgopt);
                    proposed_log_Likelihood = -nlZg;
                catch
                    % If conjugate gradient method does not converge, then reject
                    proposed_log_Likelihood = -inf;
                end
            end
            
            if isinf(proposed_log_Likelihood)
                idx_u = idx_l + 3*current_J - 1;
                samples.theta(idx_l:idx_u) = current_theta(:);
                samples.J(s+1) = current_J;
                idx_l = idx_u + 1;
                samples.gam(s+1) = current_gam;
                samples.eta(s+1) = current_eta;
                accept(s+1,:) = [0, 0, 0, 0, 0, 0];
            else
                logH = proposed_log_Likelihood - current_log_Likelihood...
                    +sum(log(proposed_theta_prior(:,end))) + log(proposed_J_prior) - log(current_J_prior)...
                    +log(p_minus + p_equal*(normcdf(epsilon/current_eta,proposed_Value(1),proposalStepSize(1))-normcdf(-epsilon/current_eta,proposed_Value(1),proposalStepSize(1)))) - log(p_plus) ...
                    -sum(log(qb));
                
                if logH > log(UH(s,1))
                    idx_u = idx_l + 3*proposed_J - 1;
                    samples.theta(idx_l:idx_u) = proposed_theta(:);
                    samples.J(s+1) = proposed_J;
                    accept(s+1,:) = [1, 1, 1, 1, 0, 0];
                    
                    current_theta = proposed_theta;
                    current_J = proposed_J;
                    idx_l = idx_u+1;
                    current_log_Likelihood = proposed_log_Likelihood;
                    current_theta_prior = proposed_theta_prior;
                    current_J_prior = proposed_J_prior;
                else
                    idx_u = idx_l + 3*current_J - 1;
                    samples.theta(idx_l:idx_u) = current_theta(:);
                    samples.J(s+1) = current_J;
                    accept(s+1, :) = [1, 0, 0, 0, 0 ,0];
                    idx_l = idx_u + 1;
                end
            end
        end
        
        if current_J > 0 && Umove(s) > p_plus && Umove(s) <= p_plus + p_equal
            % Update Step
            col = ceil(current_J*Uidx(s)); % col = r in Chong Tu's notation
            for row = 1:3
                proposed_Value = current_theta(row,col) + proposalStepSize(row)*Z(s,row);
                if row == 1 && abs(proposed_Value*current_eta) < epsilon
                    % If proposed beta is too small, skip update step and do death step
                    accept(s+1, 1) = -1;
                    Umove(s) = 1;
                    break
                end

                accept(s+1, 1) = 2;
                
                proposed_theta = current_theta;
                proposed_theta(row,col) = proposed_Value;
                proposed_BFParams = proposed_theta(2:end,:)';
                
                % Compute GP log likelihood
                if useSKI == 0
                    % Direct calculation of log likelihood if SKI parameters not provided
                    if betalogscale == 1
                        proposed_beta = exp(proposed_theta(1,:)');
                    else
                        proposed_beta = proposed_theta(1,:)';
                    end
                    if eqspace
                        K_Synth = toeplitz(BasisFunction.function_ift(tauX,proposed_BFParams)*proposed_beta) + sigma2*eye(N);
                    else
                        K_Synth = reshape(BasisFunction.function_ift(tauX(:),proposed_BFParams)*proposed_beta,N,N) + sigma2*eye(N);
                    end
                    C_Synth = cholcov(K_Synth + diag(1e-6*rand(length(K_Synth),1)));
                    if isempty(C_Synth)
                        proposed_log_Likelihood = -inf;
                    else
                        log_det = sum(log(diag(C_Synth)));
                        data_fit = y'*(C_Synth \ (C_Synth' \ y))/2;
                        proposed_log_Likelihood = -data_fit - log_det - N/2*log(2*pi);
                    end
                else
                    % SKI approximation of log-likelihood
                    gpcov = {bf_ift_wrapper,betalogscale,lambdalogscale,proposed_BFParams,proposed_theta(1,:)'};
                    covg = {@apxGrid,{gpcov},{xg}};
                    try
                        [~,nlZg] = infGrid(hyp,gpmean,covg,lik,X,y,cgopt);
                        proposed_log_Likelihood = -nlZg;
                    catch
                        % If conjugate gradient method does not converge, then reject
                        proposed_log_Likelihood = -inf;
                    end
                end
                
                if isinf(proposed_log_Likelihood)
                    idx_u = idx_l + 3*current_J - 1;
                    samples.theta(idx_l:idx_u) = current_theta(:);
                    samples.J(s+1) = current_J;
                    idx_l = idx_u + 1;
                    samples.gam(s+1) = current_gam;
                    samples.eta(s+1) = current_eta;
                    accept(s+1,:) = [0, 0, 0, 0, 0, 0];
                else
                    proposed_theta_prior = current_theta_prior;
                    proposed_theta_prior(:,col) = Levy_Prior_PDF(current_LevyPrior,BasisFunction,1,proposed_theta(:,col));
                    
                    % 4. Compute acceptance probability. Store new sample
                    H = exp(proposed_log_Likelihood - current_log_Likelihood)*proposed_theta_prior(row,col)/current_theta_prior(row,col);
                    if H > UH(s, row) % should be different random nums
                        accept(s+1, row+1) = 1;
                        current_theta_prior(row,col) = proposed_theta_prior(row,col);
                        current_log_Likelihood = proposed_log_Likelihood;
                        current_theta = proposed_theta;
                    end
                end
            end
            
            % Legit Update Step - For small proposed betas, do not do this step and
            % go straight to death step
            if Umove(s) ~= 1
                idx_u = idx_l + current_J*3 - 1;
                samples.theta(idx_l:idx_u) = current_theta(:);
                samples.J(s+1) = current_J;
                idx_l = idx_u + 1;
            end
            
        end
        
        if current_J > 0 && Umove(s) > p_plus + p_equal
            % Death Step
            col = ceil(current_J*Uidx(s));
            proposed_theta = current_theta(:,[1:col-1,col+1:end]);
            proposed_J = current_J - 1;
            proposed_BFParams = proposed_theta(2:end,:)';
            
            % Compute GP log likelihood
            if useSKI == 0
                % Direct calculation of log likelihood if SKI parameters not provided
                if betalogscale == 1
                    proposed_beta = exp(proposed_theta(1,:)');
                else
                    proposed_beta = proposed_theta(1,:)';
                end
                if eqspace
                    K_Synth = toeplitz(BasisFunction.function_ift(tauX,proposed_BFParams)*proposed_beta) + sigma2*eye(N);
                else
                    K_Synth = reshape(BasisFunction.function_ift(tauX(:),proposed_BFParams)*proposed_beta,N,N) + sigma2*eye(N);
                end
                C_Synth = cholcov(K_Synth + diag(1e-6*rand(length(K_Synth),1)));
                if isempty(C_Synth)
                    proposed_log_Likelihood = -inf;
                else
                    log_det = sum(log(diag(C_Synth)));
                    data_fit = y'*(C_Synth \ (C_Synth' \ y))/2;
                    proposed_log_Likelihood = -data_fit - log_det - N/2*log(2*pi);
                end
            else
                % SKI approximation of log-likelihood
                gpcov = {bf_ift_wrapper,betalogscale,lambdalogscale,proposed_BFParams,proposed_theta(1,:)'};
                covg = {@apxGrid,{gpcov},{xg}};
                try
                    [~,nlZg] = infGrid(hyp,gpmean,covg,lik,X,y,cgopt);
                    proposed_log_Likelihood = -nlZg;
                catch
                    % If conjugate gradient method does not converge, then reject
                    proposed_log_Likelihood = -inf;
                end
            end
            
            if isinf(proposed_log_Likelihood)
                idx_u = idx_l + 3*current_J - 1;
                samples.theta(idx_l:idx_u) = current_theta(:);
                samples.J(s+1) = current_J;
                idx_l = idx_u + 1;
                samples.gam(s+1) = current_gam;
                samples.eta(s+1) = current_eta;
                accept(s+1,:) = [0, 0, 0, 0, 0, 0];
            else
                [proposed_theta_prior, proposed_J_prior] = Levy_Prior_PDF(current_LevyPrior,BasisFunction,proposed_J,proposed_theta);
                qb = Levy_Prior_PDF(current_LevyPrior,BasisFunction,1,current_theta(:,col));
                
                logH = proposed_log_Likelihood - current_log_Likelihood...
                    - sum(log(current_theta_prior(:,col))) + log(proposed_J_prior) - log(current_J_prior)...
                    + log(p_plus) - log(p_minus + p_equal*(normcdf(epsilon/current_eta,current_theta(1,col),proposalStepSize(1))-normcdf(-epsilon/current_eta,current_theta(1,col),proposalStepSize(1)))) ...
                    + sum(log(qb));
                
                if logH > log(UH(s,1))
                    idx_u = idx_l + 3*proposed_J - 1;
                    samples.theta(idx_l:idx_u) = proposed_theta(:);
                    samples.J(s+1) = proposed_J;
                    accept(s+1, :) = [max(accept(s+1, 1), 3), 1, 1, 1, 0 ,0];
                    current_theta = proposed_theta;
                    current_J = proposed_J;
                    idx_l = idx_u+1;
                    current_log_Likelihood = proposed_log_Likelihood;
                    current_theta_prior = proposed_theta_prior;
                    current_J_prior = proposed_J_prior;
                else
                    idx_u = idx_l + 3*current_J - 1;
                    samples.theta(idx_l:idx_u) = current_theta(:);
                    samples.J(s+1) = current_J;
                    accept(s+1, :) = [max(accept(s+1, 1), 3), 0, 0, 0, 0, 0];
                    idx_l = idx_u + 1;
                end
            end
        end
        
        % Copy hyperparameters
        samples.gam(s+1) = current_gam;
        samples.eta(s+1) = current_eta;
    end
    
    samples.log_Likelihood(s+1) = current_log_Likelihood;
    samples.log_Posterior(s+1) = current_log_Likelihood + sum(log(current_theta_prior(:))) + log(current_J_prior) + log(current_hyperprior);
end